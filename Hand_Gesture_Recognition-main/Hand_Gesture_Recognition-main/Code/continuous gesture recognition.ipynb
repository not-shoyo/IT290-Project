{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\akhee\\Documents\\Projects\\NITK\\sem4\\Seminar 290\\Hand_Gesture_Recognition-main\\Hand_Gesture_Recognition-main\\Hand_Gesture_Recognition-main\\Code\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import vlc\n",
        "import os\n",
        "\n",
        "# creating the mediaplayer object and setting the file to be run on.\n",
        "# defining default values for volume and scale.\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "media_player = vlc.MediaPlayer()\n",
        "media = vlc.Media(\"video-for-executing.mkv\") \n",
        "media_player.set_media(media)\n",
        "media_player.audio_set_volume(80) \n",
        "media_player.audio_set_mute(False) \n",
        "media_player.video_set_scale(0.6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#importing required packages\n",
        "import tensorflow as tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from keras.models import Sequential,load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#importing required packages\n",
        "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
        "from keras.layers import Dense,Dropout,Flatten\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "\n",
        "bg = None\n",
        "\n",
        "#resize and save captured frames \n",
        "from PIL import Image\n",
        "def resizeImage(imageName):\n",
        "    basewidth = 100\n",
        "    img = Image.open(imageName)\n",
        "    wpercent = (basewidth/float(img.size[0]))\n",
        "    hsize = int((float(img.size[1])*float(wpercent)))\n",
        "    img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
        "    img.save(imageName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_avg(image, aWeight):\n",
        "    global bg\n",
        "    # initialize the background\n",
        "    if bg is None:\n",
        "        bg = image.copy().astype(\"float\")\n",
        "        return\n",
        "    # compute weighted average, accumulate it and update the background\n",
        "    cv2.accumulateWeighted(image, bg, aWeight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# figuring out the shape of the hand or highlighted contour part of hand from the input video\n",
        "\n",
        "def segment(image, threshold=25):\n",
        "    global bg\n",
        "    # find the absolute difference between background and current frame\n",
        "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
        "\n",
        "    # threshold the diff image so that we get the foreground\n",
        "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "    # get the contours in the thresholded image\n",
        "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # return None, if no contours detected\n",
        "    if len(cnts) == 0:\n",
        "        return\n",
        "    else:\n",
        "        # based on contour area, get the maximum contour which is the hand\n",
        "        segmented = max(cnts, key=cv2.contourArea)\n",
        "        return (thresholded, segmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "def getPredictedClass():\n",
        "    # Predict\n",
        "    image = cv2.imread('Temp.png') \n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray_image = cv2.resize(gray_image, (128,128))\n",
        "    \n",
        "    gray_image=gray_image.reshape(1,128,128,1)\n",
        "    prediction = model.predict(gray_image)\n",
        "    #calculate the probablity as a ratio between gesture of max probablity and total probablity of all gestures\n",
        "    \n",
        "    p=(np.amax(prediction) / (prediction[0][0] + prediction[0][1] + prediction[0][2] + prediction[0][3] + prediction[0][4] + prediction[0][5] + prediction[0][6] + prediction[0][7] + prediction[0][8] + prediction[0][9] ))\n",
        "    return np.argmax(prediction), (p)\n",
        "    #assigning action to gesture\n",
        "    \n",
        "def showStatistics(predictedClass, confidence):\n",
        "\n",
        "    textImage = np.zeros((300,512,3), np.uint8)\n",
        "    className = \"lmao\"\n",
        "    if predictedClass == 0:\n",
        "        className = \"Down\"\n",
        "        var=\"Volume decrease\"\n",
        "        print(className)\n",
        "        media_player.audio_set_volume(media_player.audio_get_volume()-30)\n",
        "    elif predictedClass == 1:\n",
        "        className = \"Palm\"\n",
        "        var=\"Unmute\"\n",
        "        print(className)\n",
        "        media_player.audio_set_mute(False) \n",
        "    elif predictedClass == 2:\n",
        "        className = \"L\"\n",
        "        var=\"Rewind\"\n",
        "        print(className)\n",
        "        media_player.set_time(media_player.get_time()-10000)\n",
        "    elif predictedClass == 3:\n",
        "        className = \"Fist\" \n",
        "        var=\"Mute\"\n",
        "        print(className)\n",
        "        media_player.audio_set_mute(True)\n",
        "    elif predictedClass == 4:\n",
        "        className = \"Fist_Moved\"\n",
        "        var=\"Maximize\"\n",
        "        print(className)\n",
        "        media_player.video_set_scale(1)\n",
        "    elif predictedClass == 5:\n",
        "        className = \"Thumb\" \n",
        "        var=\"Play\"\n",
        "        print(className)\n",
        "        media_player.set_pause(0)\n",
        "    elif predictedClass == 6:\n",
        "        className = \"Index\"\n",
        "        var=\"Volume increase\"\n",
        "        print(className)\n",
        "        media_player.audio_set_volume(media_player.audio_get_volume()+30)\n",
        "    elif predictedClass == 7:\n",
        "        className = \"Ok\"\n",
        "        var=\"Restore Down\"\n",
        "        print(className)\n",
        "        media_player.video_set_scale(0.6) \n",
        "    elif predictedClass == 8:\n",
        "        className = \"Palm_Moved\"\n",
        "        var=\"Pause\"\n",
        "        print(className)\n",
        "        media_player.set_pause(1)\n",
        "    elif predictedClass == 9:\n",
        "        className = \"C\"\n",
        "        var=\"Forward\"\n",
        "        print(className)\n",
        "        media_player.set_time(media_player.get_time()+10000)\n",
        "    else : \n",
        "        className = \"none\"\n",
        "        print(className)\n",
        "    cv2.putText(textImage,\"Predicted Class : \" + className, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "    cv2.putText(textImage,\"Action : \" + var, (30, 100), cv2.FONT_HERSHEY_SIMPLEX, 1,(255, 255, 255),2)\n",
        "    cv2.putText(textImage,\"Probablity : \" + str(confidence) + '%', (30, 170), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "    cv2.imshow(\"Statistics\", textImage)\n",
        "    cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    # initialize weight for running average\n",
        "    aWeight = 0.5\n",
        "    flag=0\n",
        "    t=0\n",
        "    j=0\n",
        "    m=[]\n",
        "    for i in range(0,10) :\n",
        "        m.append(0)\n",
        "    \n",
        "    # get the reference to the webcam\n",
        "    camera = cv2.VideoCapture(0)\n",
        "\n",
        "    # region of interest (ROI) coordinates\n",
        "    top, right, bottom, left = 10, 350, 225, 590\n",
        "\n",
        "    # initialize num of frames\n",
        "    num_frames = 0\n",
        "    start_recording = False\n",
        "    \n",
        "    # keep looping, until interrupted\n",
        "    while(True):\n",
        "            \n",
        "        # get the current frame\n",
        "        (grabbed, frame) = camera.read()\n",
        "\n",
        "        # resize the frame\n",
        "        frame = imutils.resize(frame, width = 700)\n",
        "\n",
        "        # flip the frame so that it is not the mirror view\n",
        "        frame = cv2.flip(frame, 1)\n",
        "\n",
        "        # clone the frame\n",
        "        clone = frame.copy()\n",
        "\n",
        "        # get the height and width of the frame\n",
        "        (height, width) = frame.shape[:2]\n",
        "\n",
        "        # get the ROI\n",
        "        roi = frame[top:bottom, right:left]\n",
        "\n",
        "        # convert the roi to grayscale and blur it\n",
        "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
        "        gray = cv2.dilate(gray, None, iterations=4)\n",
        "        # to get the background, keep looking till a threshold is reached\n",
        "        # so that our running average model gets calibrated\n",
        "        if num_frames < 30:\n",
        "            run_avg(gray, aWeight)\n",
        "            \n",
        "        else:\n",
        "            # segment the hand region\n",
        "            hand = segment(gray)\n",
        "\n",
        "            # check whether hand region is segmented\n",
        "            if hand is not None:\n",
        "                # if yes, unpack the thresholded image and\n",
        "                # segmented region\n",
        "                (thresholded, segmented) = hand\n",
        "\n",
        "                # draw the segmented region and display the frame\n",
        "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
        "                if start_recording:\n",
        "                    cv2.imwrite('Temp.png', thresholded)\n",
        "                    resizeImage('Temp.png')\n",
        "                    predictedClass, confidence = getPredictedClass()\n",
        "                    #print(predictedClass)\n",
        "                    if flag>=1 :\n",
        "                        m[predictedClass]+=1\n",
        "                        flag+=1\n",
        "                cv2.imshow(\"Thesholded\", thresholded)  \n",
        "                #for the same gesture 50 frames are taken and are predicted . Out of 50 predictions , the gesture having maximum probability is considered as final gesture \n",
        "                if flag==50 :\n",
        "                    print('in flag')\n",
        "                    t=0\n",
        "                    for i in range(0,10) :\n",
        "                        print(i,\" th = \",m[i])\n",
        "                        if m[i]>t :\n",
        "                            t=m[i]\n",
        "                            j=i;\n",
        "                    print(\"final\",t)\n",
        "                    print(\"---\",j)\n",
        "                    showStatistics(j, t*2)\n",
        "                    flag=0\n",
        "                    \n",
        "                    \n",
        "        # draw the segmented hand\n",
        "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
        "\n",
        "        # increment the number of frames\n",
        "        num_frames += 1\n",
        "\n",
        "        # display the frame with segmented hand\n",
        "        cv2.imshow(\"Video Feed\", clone)\n",
        "\n",
        "        # observe the keypress by the user\n",
        "        keypress = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "        # if the user pressed \"q\", then stop looping\n",
        "        if keypress == ord(\"q\"):\n",
        "            break\n",
        "        \n",
        "        if keypress == ord(\"s\"):\n",
        "            start_recording = True\n",
        "            time.sleep(5) \n",
        "            media_player.play() \n",
        "        \n",
        "        if keypress == ord(\" \"):\n",
        "            print(\"Gesture prediction started\")\n",
        "            flag=1\n",
        "            for i in range(0,10) :\n",
        "                m[i]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gesture prediction started\n",
            "in flag\n",
            "0  th =  0\n",
            "1  th =  0\n",
            "2  th =  0\n",
            "3  th =  1\n",
            "4  th =  0\n",
            "5  th =  0\n",
            "6  th =  0\n",
            "7  th =  0\n",
            "8  th =  48\n",
            "9  th =  0\n",
            "final 48\n",
            "--- 8\n",
            "Palm_Moved\n"
          ]
        }
      ],
      "source": [
        "import imutils\n",
        "import tensorflow\n",
        "\n",
        "# Load Saved Model\n",
        "\n",
        "model = tensorflow.keras.models.load_model(\"handgestures.h5\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
